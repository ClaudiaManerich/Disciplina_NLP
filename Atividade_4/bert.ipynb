{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67c7f4-fb27-4353-a2d6-26e42c54b1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      file_name                                               text class\n",
      "0  acq.4342.txt  mcdowell me to merger with interpharm inc mcdo...   acq\n",
      "1  acq.5302.txt  intermagnetics general inma completes buy inte...   acq\n",
      "2  acq.8530.txt  tesco extends hillards offer tesco plc tsco l ...   acq\n",
      "3  acq.3841.txt  healthvest hvt sells shares healthvest a maryl...   acq\n",
      "4  acq.6302.txt  cooper canada said it received takeover offers...   acq\n",
      "Treino: 5371, Validação: 767, Teste: 1536\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97133d66f59c4394bc7e33a9a1927d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\secad\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\secad\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\secad\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/3\n"
     ]
    }
   ],
   "source": [
    "# Importações necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "\n",
    "# 1. Carregar a base de dados\n",
    "caminho_arquivo = r\"C:\\Users\\secad\\Downloads\\treino_re8\\re8.csv\"\n",
    "dados = pd.read_csv(caminho_arquivo)\n",
    "\n",
    "# Verificar as primeiras linhas\n",
    "print(dados.head())\n",
    "\n",
    "# 2. Dividir os dados em treino (70%), validação (10%) e teste (20%)\n",
    "train_df, temp_df = train_test_split(dados, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.6667, random_state=42)  # 10% val, 20% test\n",
    "\n",
    "print(f\"Treino: {len(train_df)}, Validação: {len(val_df)}, Teste: {len(test_df)}\")\n",
    "\n",
    "# 3. Tokenização com BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Função para tokenizar os textos\n",
    "def tokenize_texts(texts, max_length=512):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "# Tokenizar os dados\n",
    "train_encodings = tokenize_texts(train_df['text'])\n",
    "val_encodings = tokenize_texts(val_df['text'])\n",
    "test_encodings = tokenize_texts(test_df['text'])\n",
    "\n",
    "# Converter as classes para tensores\n",
    "train_labels = torch.tensor(train_df['class'].astype('category').cat.codes.tolist())\n",
    "val_labels = torch.tensor(val_df['class'].astype('category').cat.codes.tolist())\n",
    "test_labels = torch.tensor(test_df['class'].astype('category').cat.codes.tolist())\n",
    "\n",
    "# Criar datasets e dataloaders\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels)\n",
    "val_dataset = TensorDataset(val_encodings['input_ids'], val_encodings['attention_mask'], val_labels)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, sampler=RandomSampler(train_dataset), batch_size=16)\n",
    "val_loader = DataLoader(val_dataset, sampler=SequentialSampler(val_dataset), batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=16)\n",
    "\n",
    "# 4. Treinar o modelo BERT\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased',\n",
    "    num_labels=len(train_df['class'].unique())\n",
    ")\n",
    "\n",
    "# Configurar otimizador\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Função de treinamento\n",
    "def train(model, train_loader, val_loader, optimizer, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Época {epoch + 1}/{epochs}\")\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Perda média: {avg_loss}\")\n",
    "        evaluate(model, val_loader)\n",
    "\n",
    "# Função de avaliação\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    return predictions, true_labels\n",
    "\n",
    "# Treinar o modelo\n",
    "train(model, train_loader, val_loader, optimizer, epochs=3)\n",
    "\n",
    "# 5. Avaliar o modelo no conjunto de teste\n",
    "predictions, true_labels = evaluate(model, test_loader)\n",
    "\n",
    "# Calcular F1-score (micro e macro), acurácia e matriz de confusão\n",
    "f1_micro = f1_score(true_labels, predictions, average='micro')\n",
    "f1_macro = f1_score(true_labels, predictions, average='macro')\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "print(f\"F1-score (Micro): {f1_micro}\")\n",
    "print(f\"F1-score (Macro): {f1_macro}\")\n",
    "print(f\"Acurácia: {accuracy}\")\n",
    "print(\"Matriz de Confusão:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934dc11a-6fee-46b3-b496-3d20a503c4e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
