{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d338af9-ac18-445c-b6bb-7b092d60cc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\secad\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\secad\\anaconda3\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\secad\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\secad\\anaconda3\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\secad\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\secad\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\secad\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\secad\\anaconda3\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\secad\\anaconda3\\lib\\site-packages (from torch) (75.4.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\secad\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\secad\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\secad\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\secad\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\secad\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\secad\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\secad\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading tiktoken-0.8.0-cp312-cp312-win_amd64.whl (883 kB)\n",
      "   ---------------------------------------- 0.0/883.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 883.8/883.8 kB 6.6 MB/s eta 0:00:00\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.8.0\n",
      "Dividindo o corpus em treino e teste...\n",
      "1765 arquivos para treino e 442 arquivos para teste.\n",
      "Extraindo texto do conjunto de treino e teste...\n",
      "Conjunto de treino final: 2754836 palavras.\n",
      "Conjunto de teste: 664729 palavras.\n",
      "Treinando o modelo Bigram...\n",
      "Treinamento concluído.\n",
      "Salvando o modelo treinado...\n",
      "Modelo salvo como 'bigram_model.pth'.\n",
      "Carregando modelo treinado...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\secad\\Downloads\\bigram_model.py:136: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo carregado com sucesso.\n",
      "\n",
      "Gerando texto aleatório:\n",
      "682 1979 — Helge escritor brasileiro (2013). * Bandeira, uma linguagem cromática e menos 719 4 **Mona atriz estado-unidenseamentos ==Demografia== População de Vagos e político presidente eleito Papa\n",
      "\n",
      "Gerando texto a partir de seed:\n",
      " multicel branco, pertenciam atirado em Paris. Ele completou um referencial pela combinação, o povoação da Ponte M % Várias possíveis no Museu Nacional de teologia === Aéficas em 0 1921, Feira é\n",
      "\n",
      "Calculando perplexidade do conjunto de teste...\n",
      "Perplexidade do conjunto de teste: 14402682.766790349\n"
     ]
    }
   ],
   "source": [
    "# Instalar bibliotecas necessárias\n",
    "!pip install tiktoken torch\n",
    "\n",
    "# Importar bibliotecas essenciais\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "# Adicionar caminho do módulo BigramModel\n",
    "sys.path.append(r\"C:\\Users\\secad\\Downloads\")  \n",
    "from bigram_model import BigramModel\n",
    "\n",
    "# Diretório do corpus\n",
    "corpus_dir = r\"C:\\Users\\secad\\Downloads\\corpus\"\n",
    "\n",
    "# Função para dividir o corpus em treino e teste\n",
    "def split_corpus(directory, train_ratio=0.8):\n",
    "    files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith(\".json\")]\n",
    "    random.shuffle(files)\n",
    "    split_index = int(len(files) * train_ratio)\n",
    "    return files[:split_index], files[split_index:]\n",
    "\n",
    "# Função para filtrar tokens raros\n",
    "def filter_rare_tokens(text, min_frequency=2):\n",
    "    tokens = text.split()\n",
    "    token_counts = {token: tokens.count(token) for token in set(tokens)}\n",
    "    return \" \".join([token for token in tokens if token_counts[token] >= min_frequency])\n",
    "\n",
    "# Função para extrair texto dos arquivos JSON\n",
    "def extract_text(files):\n",
    "    combined_text = \"\"\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            combined_text += data.get(\"text\", \"\") + \" \"\n",
    "    return combined_text.strip()\n",
    "\n",
    "# Divisão do corpus\n",
    "print(\"Dividindo o corpus em treino e teste...\")\n",
    "train_files, test_files = split_corpus(corpus_dir)\n",
    "print(f\"{len(train_files)} arquivos para treino e {len(test_files)} arquivos para teste.\")\n",
    "\n",
    "# Extração de texto do treino e teste\n",
    "print(\"Extraindo texto do conjunto de treino e teste...\")\n",
    "train_text = extract_text(train_files)\n",
    "test_text = extract_text(test_files)\n",
    "\n",
    "# Balancear o treino com uma parte do teste\n",
    "portion_of_test_to_train = \" \".join(test_text.split()[:10000])  # Pegue 10.000 palavras do teste\n",
    "train_text += \" \" + portion_of_test_to_train\n",
    "\n",
    "# Filtrar tokens raros do treino\n",
    "train_text = filter_rare_tokens(train_text)\n",
    "\n",
    "print(f\"Conjunto de treino final: {len(train_text.split())} palavras.\")\n",
    "print(f\"Conjunto de teste: {len(test_text.split())} palavras.\")\n",
    "\n",
    "# Treinamento do modelo Bigram\n",
    "print(\"Treinando o modelo Bigram...\")\n",
    "bigram = BigramModel(max_tokens=100000)  # Limite de tokens ajustado\n",
    "bigram.train(train_text)\n",
    "print(\"Treinamento concluído.\")\n",
    "\n",
    "# Salvar modelo\n",
    "print(\"Salvando o modelo treinado...\")\n",
    "bigram.save_model(\"bigram_model.pth\")\n",
    "print(\"Modelo salvo como 'bigram_model.pth'.\")\n",
    "\n",
    "# Carregar modelo\n",
    "print(\"Carregando modelo treinado...\")\n",
    "loaded_bigram = BigramModel.load_model(\"bigram_model.pth\")\n",
    "print(\"Modelo carregado com sucesso.\")\n",
    "\n",
    "# Gerar texto aleatório\n",
    "print(\"\\nGerando texto aleatório:\")\n",
    "print(loaded_bigram.generate_text(50))\n",
    "\n",
    "# Gerar texto com seed\n",
    "print(\"\\nGerando texto a partir de seed:\")\n",
    "print(loaded_bigram.generate_text(50, seed=42))\n",
    "\n",
    "# Calcular perplexidade no conjunto de teste\n",
    "print(\"\\nCalculando perplexidade do conjunto de teste...\")\n",
    "perplexity = loaded_bigram.perplexity(test_text[:500])  # Avalia os primeiros 500 tokens para teste\n",
    "print(f\"Perplexidade do conjunto de teste: {perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1eb854-eff5-40df-b2ae-4a9ef77e453e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
